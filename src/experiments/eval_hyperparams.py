"""
Hyperparameter Evaluation Utilities

Validation ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ í‰ê°€ ë° ë¹„êµ ë„êµ¬
"""

import os
import sys
import numpy as np
import torch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.insert(0, project_root)

from src.agents.dqn_agent import Qnet
from src.agents.pg_agent import PolicyNet
from src.utils.metrics import calculate_all_metrics


def evaluate_model_on_dataset(model, df, model_type='dqn'):
    """
    í•™ìŠµëœ ëª¨ë¸ì„ íŠ¹ì • ë°ì´í„°ì…‹ì—ì„œ í‰ê°€ (í™˜ê²½ ì—†ì´ ì§ì ‘ ê³„ì‚°)
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸ (Qnet ë˜ëŠ” PolicyNet)
        df: í‰ê°€í•  ë°ì´í„°í”„ë ˆì„ (train/val/test)
        model_type: 'dqn' ë˜ëŠ” 'pg'
        
    Returns:
        dict: {
            'sharpe_ratio': float,
            'annualized_return': float,
            'volatility': float,
            'max_drawdown': float,
            'cumulative_return': float,
            'returns': np.array (ì¼ë³„ ìˆ˜ìµë¥ )
        }
    """
    model.eval()  # í‰ê°€ ëª¨ë“œ
    
    # ì´ˆê¸° ìƒíƒœ
    w_stock = 0.5
    w_bond = 0.5
    returns = []
    prev_month = None
    
    # ì•¡ì…˜ ê°’ ë°°ì—´ (í™˜ê²½ê³¼ ë™ì¼)
    action_values = np.linspace(-1.0, 1.0, 41)
    
    # ìµœì†Œ 5ì¼ ì´í›„ë¶€í„° ì‹œì‘ (ê³¼ê±° 5ì¼ ë°ì´í„° í•„ìš”)
    with torch.no_grad():
        for i in range(5, len(df)):
            # 1. ìƒíƒœ ìƒì„±
            r_stock_history = df['r_stock'].iloc[i-4:i+1].values  # 5ê°œ
            r_bond_history = df['r_bond'].iloc[i-4:i+1].values    # 5ê°œ
            
            state = np.concatenate([
                r_stock_history,  # 5ê°œ
                r_bond_history,   # 5ê°œ
                [w_stock]         # 1ê°œ
            ]).astype(np.float32)
            
            # 2. ëª¨ë¸ë¡œ ì•¡ì…˜ ì„ íƒ
            if model_type == 'dqn':
                state_tensor = torch.from_numpy(state).float().unsqueeze(0)
                q_values = model(state_tensor)
                action = q_values.argmax().item()
            else:  # pg
                # PG: í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ì•¡ì…˜ ì„ íƒ (deterministic evaluation)
                state_tensor = torch.from_numpy(state).float()
                action_probs = model(state_tensor)
                action = action_probs.argmax().item()
            
            # 3. ì•¡ì…˜ì„ delta_wë¡œ ë³€í™˜
            delta_w = action_values[action]
            
            # 4. ì›”ì´ˆ ë¦¬ë°¸ëŸ°ì‹±
            current_month = df.index[i].month
            if prev_month is not None and current_month != prev_month:
                # ì›”ì´ ë°”ë€œ â†’ ë¦¬ë°¸ëŸ°ì‹±
                w_stock = np.clip(w_stock + delta_w, 0.0, 1.0)
                w_bond = 1.0 - w_stock
            prev_month = current_month
            
            # 5. í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê³„ì‚°
            r_portfolio = w_stock * df['r_stock'].iloc[i] + w_bond * df['r_bond'].iloc[i]
            returns.append(r_portfolio)
    
    # 6. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    metrics = calculate_all_metrics(returns)
    
    # 7. ìˆ˜ìµë¥  ë°°ì—´ ì¶”ê°€
    metrics['returns'] = np.array(returns)
    
    return metrics


def load_model(model_path, model_type='dqn'):
    """
    ëª¨ë¸ íŒŒì¼ ë¡œë“œ
    
    Args:
        model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        model_type: 'dqn' ë˜ëŠ” 'pg'
        
    Returns:
        model: ë¡œë“œëœ ëª¨ë¸
    """
    checkpoint = torch.load(model_path, weights_only=False)
    
    if model_type == 'dqn':
        model = Qnet()
        model.load_state_dict(checkpoint['model_state_dict'])
    else:  # pg
        model = PolicyNet()
        model.load_state_dict(checkpoint['policy_state_dict'])
    
    model.eval()
    return model


def compare_models_on_validation(model_paths, val_df, model_type='dqn'):
    """
    ì—¬ëŸ¬ ëª¨ë¸ì„ Validation ë°ì´í„°ì—ì„œ ë¹„êµ
    
    Args:
        model_paths: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ {ì´ë¦„: ê²½ë¡œ}
        val_df: Validation ë°ì´í„°í”„ë ˆì„
        model_type: 'dqn' ë˜ëŠ” 'pg'
        
    Returns:
        results: {ëª¨ë¸ëª…: metrics} ë”•ì…”ë„ˆë¦¬
        best_model: (ëª¨ë¸ëª…, ê²½ë¡œ, Sharpe ratio) íŠœí”Œ
    """
    results = {}
    
    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    if isinstance(model_paths, list):
        model_paths = {os.path.basename(path): path for path in model_paths}
    
    print("=" * 70)
    print(f"Comparing {len(model_paths)} models on Validation set")
    print("=" * 70)
    
    for model_name, model_path in model_paths.items():
        print(f"\nEvaluating: {model_name}")
        print("-" * 70)
        
        # ëª¨ë¸ ë¡œë“œ
        model = load_model(model_path, model_type)
        
        # Validation í‰ê°€
        metrics = evaluate_model_on_dataset(model, val_df, model_type)
        
        # ê²°ê³¼ ì €ì¥
        results[model_name] = metrics
        
        # ì£¼ìš” ì§€í‘œ ì¶œë ¥
        print(f"  Sharpe Ratio:       {metrics['sharpe_ratio']:>8.4f}")
        print(f"  Annualized Return:  {metrics['annualized_return']:>8.2%}")
        print(f"  Volatility:         {metrics['volatility']:>8.2%}")
        print(f"  Max Drawdown:       {metrics['max_drawdown']:>8.2%}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
    best_model_name = max(results.items(), key=lambda x: x[1]['sharpe_ratio'])
    best_model_path = model_paths[best_model_name[0]]
    
    print("\n" + "=" * 70)
    print(f"ğŸ† Best Model: {best_model_name[0]}")
    print(f"   Sharpe Ratio: {best_model_name[1]['sharpe_ratio']:.4f}")
    print("=" * 70)
    
    return results, (best_model_name[0], best_model_path, best_model_name[1]['sharpe_ratio'])


def print_comparison_table(results, title="Model Comparison"):
    """
    ëª¨ë¸ ë¹„êµ í…Œì´ë¸” ì¶œë ¥
    
    Args:
        results: {ëª¨ë¸ëª…: metrics} ë”•ì…”ë„ˆë¦¬
        title: í…Œì´ë¸” ì œëª©
    """
    print("\n" + "=" * 90)
    print(f"{title}")
    print("=" * 90)
    
    # í—¤ë”
    print(f"{'Model':<30} {'Sharpe':<10} {'Ann.Ret':<10} {'Vol':<10} {'MaxDD':<10}")
    print("-" * 90)
    
    # ê° ëª¨ë¸ ì¶œë ¥ (Sharpe ratio ê¸°ì¤€ ì •ë ¬)
    sorted_results = sorted(results.items(), key=lambda x: x[1]['sharpe_ratio'], reverse=True)
    
    for model_name, metrics in sorted_results:
        print(f"{model_name:<30} "
              f"{metrics['sharpe_ratio']:<10.4f} "
              f"{metrics['annualized_return']:<10.2%} "
              f"{metrics['volatility']:<10.2%} "
              f"{metrics['max_drawdown']:<10.2%}")
    
    print("=" * 90)


if __name__ == '__main__':
    """
    í…ŒìŠ¤íŠ¸ ì½”ë“œ
    """
    from src.utils.data_loader import load_sp500_data, load_dgs10_data, preprocess_data, split_data
    from src.config import DATA_START_DATE, DATA_END_DATE
    
    print("Testing eval_hyperparams.py")
    print("=" * 70)
    
    # ë°ì´í„° ë¡œë”©
    print("\nLoading data...")
    sp500_df = load_sp500_data(start_date=DATA_START_DATE, end_date=DATA_END_DATE)
    dgs10_df = load_dgs10_data()
    df = preprocess_data(sp500_df, dgs10_df)
    train_df, val_df, test_df = split_data(df)
    
    print(f"âœ“ Validation: {len(val_df)} days ({val_df.index[0].date()} ~ {val_df.index[-1].date()})")
    
    # ëª¨ë¸ ì°¾ê¸°
    models_dir = os.path.join(project_root, 'results', 'models')
    
    if os.path.exists(models_dir):
        dqn_models = [f for f in os.listdir(models_dir) if f.startswith('dqn_') and f.endswith('.pt')]
        pg_models = [f for f in os.listdir(models_dir) if f.startswith('pg_') and f.endswith('.pt')]
        
        print(f"\nFound {len(dqn_models)} DQN models and {len(pg_models)} PG models")
        
        if dqn_models:
            print("\nTesting with first DQN model...")
            model_path = os.path.join(models_dir, dqn_models[0])
            model = load_model(model_path, 'dqn')
            metrics = evaluate_model_on_dataset(model, val_df, 'dqn')
            print(f"âœ“ Validation Sharpe: {metrics['sharpe_ratio']:.4f}")
    else:
        print(f"\nâš ï¸  No models directory found at {models_dir}")
        print("   Train some models first!")
